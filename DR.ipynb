{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarthCipher7/DR/blob/main/DR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 1: Mount Drive and Unzip Data ###\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount your Google Drive (this will require authentication)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to your zip file\n",
        "zip_path = '/content/drive/MyDrive/Colab Notebooks/data.zip'\n",
        "\n",
        "# Define the directory to extract the data to\n",
        "extract_path = '/content/data'\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Unzip your data\n",
        "print(\"Starting to unzip the data...\")\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "print(\"Data unzipped successfully!\")\n",
        "\n",
        "# You can add this to see the contents of your unzipped folder\n",
        "print(\"\\nContents of the data folder:\")\n",
        "!ls /content/data"
      ],
      "metadata": {
        "id": "JKmnqm_cJOHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### MODIFIED CELL 2: Load Labels and Prepare Data Generators (with Augmentation and Defined Order) ###\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "main_data_dir = '/content/data/data/gaussian_filtered_images/gaussian_filtered_images'\n",
        "\n",
        "# Define the class order based on severity\n",
        "# IMPORTANT: This assumes your subdirectories are named '0', '1', '2', '3', '4'\n",
        "# If they have names like 'Mild', 'Moderate', etc., list them in the desired order.\n",
        "CLASS_ORDER = ['Mild', 'Moderate', 'No_DR', 'Proliferate_DR', 'Severe']\n",
        "\n",
        "# This is for the training data - we apply lots of random transformations\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2,    # Still split the data\n",
        "    rotation_range=20,       # Randomly rotate images\n",
        "    width_shift_range=0.1,   # Randomly shift images horizontally\n",
        "    height_shift_range=0.1,  # Randomly shift images vertically\n",
        "    shear_range=0.1,         # Apply shear transformations\n",
        "    zoom_range=0.1,          # Randomly zoom in on images\n",
        "    horizontal_flip=True,    # Randomly flip images horizontally\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# This is for the validation data - we ONLY rescale it.\n",
        "validation_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# --- Create the generators using the new datagens and defined class order ---\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    directory=main_data_dir,\n",
        "    target_size=(300, 300), # EfficientNetB3 works well with 300x300 images\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    classes=CLASS_ORDER, # Enforce the class order\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    directory=main_data_dir,\n",
        "    target_size=(300, 300), # Use the same size for validation\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    classes=CLASS_ORDER, # Enforce the class order\n",
        "    shuffle=False # DO NOT shuffle validation data for evaluation\n",
        ")\n"
      ],
      "metadata": {
        "id": "cB_WAxzqJO2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### ADDED CELL 2a: Calculate Class Weights to Handle Imbalance ###\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Get the class labels from the generator\n",
        "# These will be sorted alphabetically by folder name (0, 1, 2, 3, 4)\n",
        "class_labels = sorted(train_generator.class_indices.keys())\n",
        "\n",
        "# Get the class indices for the training data\n",
        "training_classes = train_generator.classes\n",
        "\n",
        "# Calculate class weights\n",
        "# The 'balanced' mode automatically adjusts weights inversely proportional to class frequencies\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(training_classes),\n",
        "    y=training_classes\n",
        ")\n",
        "\n",
        "# Create a dictionary mapping class indices to their calculated weights\n",
        "# The model.fit() function expects the weights in this format\n",
        "class_weight_dict = dict(zip(np.unique(training_classes), class_weights))\n",
        "\n",
        "print(\"Class labels are:\", class_labels)\n",
        "print(\"Calculated class weights are:\", class_weights)\n",
        "print(\"Class weight dictionary for the model:\", class_weight_dict)\n"
      ],
      "metadata": {
        "id": "SpwpyB5SJSOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### UPGRADED CELL 3: Build the Model with EfficientNetB3 ###\n",
        "\n",
        "from tensorflow.keras.applications import EfficientNetB3\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# --- Step 1: Load the EfficientNetB3 base model ---\n",
        "# EfficientNet is a more modern and powerful architecture\n",
        "base_model = EfficientNetB3(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(300, 300, 3)\n",
        ")\n",
        "\n",
        "# --- Step 2: Allow the whole base model to be fine-tuned ---\n",
        "# This allows the model to better adapt its learned features to our specific task\n",
        "base_model.trainable = True\n",
        "\n",
        "# --- Step 3: Build the final model ---\n",
        "# Using GlobalAveragePooling2D is often more effective than Flatten\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(), # Reduces dimensions and parameters\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(5, activation='softmax')  # Output layer with 5 units for 5 classes\n",
        "])\n"
      ],
      "metadata": {
        "id": "cXUV396kJYNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 4: Compile the model with a slightly higher initial learning rate ---\n",
        "# We can start a bit higher because we will use a learning rate scheduler\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4), # Start with 1e-4\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Print a summary to see the new architecture\n",
        "model.summary()\n",
        "\n",
        "\n",
        "### UPGRADED CELL 4: Train the Model with an Advanced Callback ###\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Stop training if val_loss doesn't improve for 5 epochs\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Save the best model found\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    'best_retinopathy_efficientnet_model.keras',\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# *** NEW: Reduce learning rate when a metric has stopped improving ***\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5, # Reduce LR by half\n",
        "    patience=2, # If val_loss doesn't improve for 2 epochs\n",
        "    min_lr=1e-6, # Don't let the LR go too low\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# --- Train the model with the new callback and class weights ---\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=30,\n",
        "    validation_data=validation_generator,\n",
        "    # ADD all callbacks here\n",
        "    callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
        "    class_weight=class_weight_dict\n",
        ")\n"
      ],
      "metadata": {
        "id": "Z5wBKdt2JakF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 5: Evaluate the Model ###\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the training and validation accuracy\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plot the training and validation loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Load your best fine-tuned model\n",
        "final_model = tf.keras.models.load_model('best_retinopathy_efficientnet_model.keras')\n",
        "\n",
        "# Evaluate its performance on the validation set\n",
        "print(\"Evaluating the final model...\")\n",
        "final_scores = final_model.evaluate(validation_generator)\n",
        "\n",
        "print(f\"\\nFinal Validation Loss: {final_scores[0]}\")\n",
        "print(f\"Final Validation Accuracy: {final_scores[1]}\")\n"
      ],
      "metadata": {
        "id": "RZfjtT5BJec_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### ADDED CELL 6: Generate and Visualize the Confusion Matrix ###\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Ensure the validation generator is not shuffled to keep labels in order\n",
        "validation_generator.shuffle = False\n",
        "\n",
        "# Predict the classes for the validation set\n",
        "Y_pred = final_model.predict(validation_generator)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "# Get the true classes\n",
        "y_true = validation_generator.classes\n",
        "\n",
        "# Get the class labels from the generator\n",
        "class_labels = list(validation_generator.class_indices.keys())\n",
        "\n",
        "# --- Generate the Confusion Matrix ---\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# --- Plot the Confusion Matrix ---\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_labels, yticklabels=class_labels)\n",
        "\n",
        "plt.title('Confusion Matrix', fontsize=16)\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# --- Print a Detailed Classification Report ---\n",
        "# This report provides precision, recall, and f1-score for each class\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Classification Report\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_labels))\n",
        "\n",
        "\n",
        "\n",
        "# Load your best fine-tuned model\n",
        "final_model = tf.keras.models.load_model('best_retinopathy_efficientnet_model.keras')\n",
        "\n"
      ],
      "metadata": {
        "id": "sLzXyI7DJhET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate its performance on the validation set\n",
        "print(\"Evaluating the final model...\")\n",
        "final_scores = final_model.evaluate(validation_generator)\n",
        "\n",
        "print(f\"\\nFinal Validation Loss: {final_scores[0]}\")\n",
        "print(f\"Final Validation Accuracy: {final_scores[1]}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Lyv0lN4oJo53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cell 8\n",
        "\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define IMG_SIZE\n",
        "IMG_SIZE = (300, 300) # Changed from (224, 224) to match model input\n",
        "\n",
        "# --- Load the saved model from Drive ---\n",
        "# predictor_model = tf.keras.models.load_model(model_save_path) # Removed this line\n",
        "predictor_model = final_model # Use the model loaded in the previous cell\n",
        "\n",
        "# --- Get class names from the generator ---\n",
        "class_indices = train_generator.class_indices\n",
        "class_labels = {v: k for k, v in class_indices.items()}\n",
        "\n",
        "# --- Upload file ---\n",
        "print(\"Please upload a retinal image for prediction:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# --- Process and predict each uploaded file ---\n",
        "for fn in uploaded.keys():\n",
        "  # Load and prepare the image\n",
        "  path = '/content/' + fn\n",
        "  img = load_img(path, target_size=IMG_SIZE)\n",
        "  img_array = img_to_array(img)\n",
        "  img_array /= 255.0\n",
        "  img_batch = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "  # Make prediction\n",
        "  prediction = predictor_model.predict(img_batch)\n",
        "  predicted_class_index = np.argmax(prediction[0])\n",
        "  predicted_class_label = class_labels[predicted_class_index]\n",
        "  confidence = np.max(prediction[0]) * 100\n",
        "\n",
        "  # Display results\n",
        "  plt.figure(figsize=(6, 7))\n",
        "  plt.imshow(img)\n",
        "  plt.axis('off')\n",
        "  plt.title(f\"Predicted Class: {predicted_class_label}\\nConfidence: {confidence:.2f}%\", fontsize=14)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "zgNZK9BHJp5U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}